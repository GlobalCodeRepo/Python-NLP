sentence-transformers>=2.2.2
torch>=2.0.0

openai>=1.6.0
streamlit>=1.32.0
python-dotenv>=1.0.0
sentence-transformers>=2.2.2
torch>=2.0.0
# ... (other existing packages you already have)

===================================================
# backend/llm_client.py
from __future__ import annotations

import os
import json
from typing import Any, Dict, List

from openai import AzureOpenAI
from sentence_transformers import SentenceTransformer

# -----------------------------
# Azure OpenAI (chat only)
# -----------------------------
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
AZURE_DEPLOYMENT_CHAT = os.getenv("AZURE_DEPLOYMENT_CHAT")

azure_client = AzureOpenAI(
    api_key=AZURE_OPENAI_API_KEY,
    api_version=AZURE_OPENAI_API_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
)

# -----------------------------
# Local embeddings (open-source)
# -----------------------------
# This will be downloaded automatically the first time you run it
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)


def embed_texts(texts: List[Any]) -> List[List[float]]:
    """
    Compute sentence embeddings locally using an open-source model.
    This replaces Azure embeddings completely.

    Input:  list of arbitrary text-like objects
    Output: list of embedding vectors (lists of floats)
    """
    clean: List[str] = []

    for t in texts:
        if t is None:
            continue

        if isinstance(t, bytes):
            s = t.decode("utf-8", errors="ignore")
        else:
            s = str(t)

        s = s.replace("\x00", "")
        if s.strip():
            clean.append(s)

    if not clean:
        return []

    # encode returns a numpy array; convert to nested Python lists
    vecs = embedding_model.encode(
        clean,
        normalize_embeddings=True,       # cosine similarity-friendly
        convert_to_numpy=True,
    )

    return vecs.tolist()


def chat_json(system_prompt: str, user_payload: Any, max_tokens: int = 4000) -> Dict[str, Any]:
    """
    Call Azure OpenAI chat completion, expecting *JSON* output.
    This part is unchanged – still uses your Azure chat deployment.
    """
    resp = azure_client.chat.completions.create(
        model=AZURE_DEPLOYMENT_CHAT,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": json.dumps(user_payload)},
        ],
        max_tokens=max_tokens,
        temperature=0.1,
    )
    text = resp.choices[0].message.content

    print("\n---- RAW LLM RESPONSE ----\n", text)

    try:
        return json.loads(text)
    except Exception:
        # try to salvage JSON substring if model adds extra text
        start = text.find("{")
        end = text.rfind("}")
        if start != -1 and end != -1:
            return json.loads(text[start : end + 1])
        raise

============================================================================

# backend/llm_client.py
from __future__ import annotations
from typing import List

from sentence_transformers import SentenceTransformer

# ---------------------------------------------------------
# Load embedding model ONCE at import time
# ---------------------------------------------------------
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

print(f"[Embedding] Loading local model: {EMBEDDING_MODEL_NAME} ...")

embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)

print("[Embedding] Model loaded successfully.")

# ---------------------------------------------------------
# Embedding wrapper function
# ---------------------------------------------------------
def embed_texts(texts: List[str]) -> List[List[float]]:
    """
    Takes a list of strings and returns a list of embedding vectors.
    Uses local SentenceTransformer embeddings.
    """
    if not texts:
        return []

    # SentenceTransformer returns a numpy array — convert to Python lists
    embeddings = embedding_model.encode(texts, convert_to_numpy=True).tolist()
    return embeddings









###############

# backend/llm_client.py
from __future__ import annotations

import os
import json
from pathlib import Path
from typing import Any, Dict, List

from openai import AzureOpenAI
from sentence_transformers import SentenceTransformer


# ---------------------------------------------------------
# Azure OpenAI (CHAT ONLY)
# ---------------------------------------------------------
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
AZURE_DEPLOYMENT_CHAT = os.getenv("AZURE_DEPLOYMENT_CHAT")

azure_client = AzureOpenAI(
    api_key=AZURE_OPENAI_API_KEY,
    api_version=AZURE_OPENAI_API_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
)


# ---------------------------------------------------------
# Local embeddings via SentenceTransformer
# ---------------------------------------------------------
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

print(f"[Embedding] Loading local model: {EMBEDDING_MODEL_NAME} ...")
embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
print("[Embedding] Model loaded successfully.")


def embed_texts(texts: List[Any]) -> List[List[float]]:
    """
    Compute embeddings locally using an open-source model.
    This completely replaces Azure embeddings.

    Input:  list of arbitrary text-like objects
    Output: list of embedding vectors (lists of floats)
    """
    clean: List[str] = []

    for t in texts:
        if t is None:
            continue

        if isinstance(t, bytes):
            s = t.decode("utf-8", errors="ignore")
        else:
            s = str(t)

        s = s.replace("\x00", "")
        if s.strip():
            clean.append(s)

    if not clean:
        return []

    vecs = embedding_model.encode(
        clean,
        normalize_embeddings=True,
        convert_to_numpy=True,
    )

    return vecs.tolist()


# ---------------------------------------------------------
# Chat → JSON with robust fallback
# ---------------------------------------------------------
def chat_json(system_prompt: str, user_payload: Any, max_tokens: int = 4000) -> Dict[str, Any]:
    """
    Call Azure chat model and try to parse the response as JSON.
    If JSON is malformed, return a safe fallback structure instead
    of crashing.
    """
    resp = azure_client.chat.completions.create(
        model=AZURE_DEPLOYMENT_CHAT,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": json.dumps(user_payload)},
        ],
        temperature=0.1,
        max_tokens=max_tokens,
    )

    text = resp.choices[0].message.content

    # Save raw response for debugging
    try:
        Path("backend/data").mkdir(parents=True, exist_ok=True)
        with open("backend/data/last_llm_response.txt", "w", encoding="utf-8") as f:
            f.write(text)
    except Exception:
        pass

    # 1) Try direct JSON parse
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    # 2) Try to salvage the substring between the first '{' and last '}'
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1 and end > start:
        candidate = text[start : end + 1]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass

    # 3) FINAL FALLBACK:
    #    Return a safe dict that has the expected keys so the rest of the
    #    pipeline can continue instead of crashing.
    return {
        "doc": {
            "raw_text": text,
            "note": "LLM did not return valid JSON; raw text captured instead.",
        },
        "validation": {
            "status": "json_parse_failed",
            "message": "Fallback used because JSON decoding failed.",
        },
    }
=======================================================================================÷

def _doc_system_prompt() -> str:
    """
    System prompt for documentation generation.

    We want a STRICT, EXHAUSTIVE, CLASS-CENTRIC description that can later be
    used to generate a full Spring Boot application. The model MUST NOT skip
    classes, methods, parameters or important control flows.
    """
    return """
You are an expert software engineer and technical writer.

You are given:
- A CHUNK of legacy Java source code.
- Metadata from the AST (file path, class / method ranges, etc.).

Your task is to produce STRICT, MACHINE-READABLE documentation for this chunk.

### General requirements

1. Do NOT skip content.
   - If a class, inner class, enum, interface, method or field appears in the chunk,
     it MUST appear in the documentation.
   - If a method body is partly outside the chunk due to truncation, still document
     what is visible and mark it as "possibly_partial": true.

2. Be CONSISTENT across chunks.
   - Use the same class_name, method_name, field_name each time.
   - Do NOT invent or rename classes or methods.

3. Capture RELATIONSHIPS.
   - For each class, list:
     - other classes it uses (calls, instantiates, or depends on),
     - other classes that use it (if obvious from the chunk),
     - external systems (files, DB, network, console, etc.).

4. No prose outside JSON.
   - Output MUST be a single JSON object.
   - No markdown, no backticks, no commentary.
   - If something is unclear, represent the uncertainty inside fields such as
     "notes" or "possibly_partial": true.

### Output JSON shape

You MUST return a JSON object with this shape (all keys required):

{
  "chunk_id": "<string – we will fill in>",
  "classes": [
    {
      "name": "<class or inner-class name>",
      "kind": "class | interface | enum",
      "package": "<package or 'default'>",
      "is_inner": true | false,
      "outer_class": "<name of outer class or null>",
      "description": "<short high-level description>",
      "fields": [
        {
          "name": "<field name>",
          "type": "<type as in code>",
          "description": "<what this field represents>",
          "is_static": true | false,
          "is_final": true | false
        }
      ],
      "methods": [
        {
          "name": "<method name>",
          "description": "<purpose of the method>",
          "return_type": "<type>",
          "parameters": [
            {
              "name": "<parameter name>",
              "type": "<type>",
              "description": "<purpose>"
            }
          ],
          "throws": ["<exception types>"],
          "side_effects": [
            "reads input", "writes to console", "writes to file", "DB access",
            "calls external service", "none", ...
          ],
          "important_flows": [
            "<short bullet-style descriptions of key branches or loops>"
          ],
          "possibly_partial": true | false
        }
      ],
      "uses_classes": ["<names of classes it calls or depends on>"],
      "used_by_hint": ["<names of classes that appear to use this class>"],
      "notes": "<any extra technical notes, limitations, TODOs>"
    }
  ]
}
"""

================================================================================
# backend/blueprint_converter.py
from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Any, Dict, List, Tuple

from openai import AzureOpenAI

from .llm_client import chat_json  # robust JSON helper


class BlueprintSpringBootConverter:
    """
    Uses the collated documentation for a legacy file to:
      1. Build a Spring Boot architecture blueprint.
      2. Generate multiple Spring Boot classes (controller, service, repository,
         model, util, config) that follow that blueprint.
    """

    def __init__(self, output_root: Path = Path("backend/generated")) -> None:
        self.output_root = output_root
        self.chat_client = AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        )
        self.chat_model = os.getenv("AZURE_DEPLOYMENT_CHAT")

    # ------------------------------------------------------------------
    # PUBLIC API
    # ------------------------------------------------------------------

    def convert_file(
        self,
        file_doc: Dict[str, Any],
        original_file_name: str,
        domain_name: str = "ChequeProcessing",
    ) -> Tuple[Dict[str, Any], List[Path]]:
        """
        High-level wrapper:
          - builds blueprint
          - generates Java classes for all modules
          - returns the blueprint and list of generated files.
        """
        blueprint = self._build_blueprint(file_doc, original_file_name, domain_name)
        java_files = self._generate_java_classes(file_doc, blueprint)
        return blueprint, java_files

    # ------------------------------------------------------------------
    # STEP 1: ARCHITECTURE BLUEPRINT
    # ------------------------------------------------------------------

    def _build_blueprint(
        self,
        file_doc: Dict[str, Any],
        original_file_name: str,
        domain_name: str,
    ) -> Dict[str, Any]:
        """
        Ask the LLM to design a Spring Boot architecture that covers ALL
        documented classes, not just a subset.
        """
        system_prompt = """
You are an expert solution architect specializing in Spring Boot modernization
of large monolithic Java applications for financial services.

You will be given machine-generated documentation for a legacy Java file.
The documentation contains classes, methods, relationships and side-effects.

Your task in THIS STEP is ONLY to design a HIGH-LEVEL SPRING BOOT ARCHITECTURE
BLUEPRINT. Do NOT generate Java code here.

The blueprint MUST:

- Cover ALL documented classes that represent meaningful behavior or data.
- Explicitly map each legacy class into one or more Spring Boot components
  (controller, service, repository, model, util, config).
- Preserve important flows like:
    - user login / authentication
    - cheque creation, validation, printing, batch processing
    - currency exchange
    - reporting (history, status, exception report)

Return STRICT JSON with this shape:

{
  "root_package": "com.bank.cheque",
  "modules": {
    "controller": [
      {
        "name": "<ControllerClassName>",
        "source_classes": ["CreateApplication", "UserService", "..."],
        "responsibilities": ["..."],
        "exposes_endpoints_for": ["login", "processCheque", "generateReport"]
      }
    ],
    "service": [
      {
        "name": "<ServiceClassName>",
        "source_classes": ["UserService", "ChequeProcessor", "..."],
        "depends_on": ["UserRepository", "ChequeRepository"],
        "key_operations": ["login", "validateCheque", "..."]
      }
    ],
    "repository": [
      {
        "name": "<RepositoryClassName>",
        "source_classes": ["User", "Cheque", "..."],
        "entity": "<EntityName>",
        "persistence_type": "in_memory | jdbc | jpa | file | other"
      }
    ],
    "model": [
      {
        "name": "<EntityOrDTOName>",
        "source_classes": ["User", "Cheque", "CurrencyRate", "..."],
        "fields": ["field1", "field2", "..."],
        "used_by_services": ["UserService", "ChequeService"]
      }
    ],
    "util": [
      {
        "name": "<UtilityClassName>",
        "source_classes": ["Logger", "EmailNotificationService", "..."],
        "purpose": "<short description>"
      }
    ],
    "config": [
      {
        "name": "<ConfigClassName>",
        "purpose": "<short description>"
      }
    ]
  },
  "cross_cutting_concerns": [
    "logging", "exception handling", "input validation", ...
  ],
  "critical_flows": [
    {
      "name": "Cheque processing",
      "from": "ControllerClass.method",
      "through": ["ServiceClass.method", "RepositoryClass.method"],
      "notes": "..."
    }
  ]
}

Rules:
- DO NOT invent random features. Stay faithful to the documentation.
- Use source_classes to indicate which legacy classes feed each Spring component.
- Try to include ALL important legacy classes somewhere in the modules.
"""

        payload = {
            "file_name": original_file_name,
            "domain_name": domain_name,
            "classes": file_doc.get("classes", []),
        }

        blueprint = chat_json(system_prompt, payload, max_tokens=3000)
        return blueprint

    # ------------------------------------------------------------------
    # STEP 2: PER-MODULE JAVA CLASS GENERATION
    # ------------------------------------------------------------------

    def _generate_java_classes(
        self, file_doc: Dict[str, Any], blueprint: Dict[str, Any]
    ) -> List[Path]:
        root_package = blueprint.get("root_package", "com.bank.legacyconv")
        modules = blueprint.get("modules", {})

        generated: List[Path] = []

        for role in ["model", "repository", "service", "controller", "util", "config"]:
            for entry in modules.get(role, []):
                java_text = self._generate_one_class(root_package, role, entry, file_doc)
                java_path = self._write_java_file(root_package, role, entry["name"], java_text)
                generated.append(java_path)

        return generated

    def _write_java_file(
        self, root_package: str, role: str, class_name: str, java_text: str
    ) -> Path:
        package_path = f"{root_package}.{role}".replace(".", "/")
        java_dir = self.output_root / "src" / "main" / "java" / package_path
        java_dir.mkdir(parents=True, exist_ok=True)
        java_path = java_dir / f"{class_name}.java"
        java_path.write_text(java_text, encoding="utf-8")
        return java_path

    # ------------------------------------------------------------------
    # Single-class generation
    # ------------------------------------------------------------------

    def _generate_one_class(
        self,
        root_package: str,
        role: str,
        blueprint_entry: Dict[str, Any],
        file_doc: Dict[str, Any],
    ) -> str:
        """
        Ask the LLM to generate one Spring Boot class for the given role
        using the subset of documentation relevant to this component.
        """
        class_name = blueprint_entry["name"]
        source_classes = blueprint_entry.get("source_classes", [])

        relevant_docs = self._collect_relevant_docs(file_doc, source_classes)

        system_prompt = """
You are a senior Spring Boot engineer.

You will generate ONE Java class that is part of a larger Spring Boot
application for cheque processing.

You are given:
- root_package: root package name for the application
- role: one of "controller", "service", "repository", "model", "util", "config"
- target: blueprint entry for this class
- source_docs: documentation for the legacy classes that this component should
  implement or adapt.

You MUST:

1. Generate EXACTLY ONE public Java class with the name given in target.name.
2. Place it in the package: root_package + "." + role.
3. Use appropriate Spring annotations:
   - controller  -> @RestController (and optionally @RequestMapping)
   - service     -> @Service
   - repository  -> @Repository (or simple in-memory repository)
   - model       -> plain POJO with fields, getters/setters, equals/hashCode/toString
   - util        -> @Component or simple utility class
   - config      -> @Configuration (if appropriate)
4. Implement as much real logic as possible based on source_docs:
   - preserve method names and parameters where they map cleanly.
   - implement main flows such as login, cheque validation, batch processing,
     currency exchange, report generation, etc.
   - if full implementation would be too large or unclear, add TODO comments
     but still implement the skeleton with key steps and data transformations.
5. For repositories, use in-memory collections (e.g. Map<String, User>) for this POC.
6. Handle errors and log important steps with System.out.println (or a simple logger).
7. Output ONLY raw Java code. No markdown, no backticks, no prose.

Code must be compilable with Java 11 and typical Spring Boot dependencies.
"""

        payload = {
            "root_package": root_package,
            "role": role,
            "target": blueprint_entry,
            "source_docs": relevant_docs,
        }

        resp = self.chat_client.chat.completions.create(
            model=self.chat_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": json.dumps(payload)},
            ],
            temperature=0.2,
            max_tokens=3500,
        )

        java_code = resp.choices[0].message.content or ""
        return java_code

    # ------------------------------------------------------------------
    # Helper: select relevant documentation for a blueprint entry
    # ------------------------------------------------------------------

    def _collect_relevant_docs(
        self,
        file_doc: Dict[str, Any],
        source_classes: List[str],
    ) -> List[Dict[str, Any]]:
        """
        Filter the full documentation to only classes related to this component.
        Uses source_classes as the primary signal, but also pulls in classes
        they depend on (uses_classes).
        """
        all_classes = file_doc.get("classes", [])
        by_name = {c.get("name"): c for c in all_classes}

        result: Dict[str, Dict[str, Any]] = {}

        # start with explicit source_classes
        for name in source_classes:
            if name in by_name:
                result[name] = by_name[name]

        # include directly used classes
        to_expand = list(result.values())
        for c in to_expand:
            for used in c.get("uses_classes", []):
                if used in by_name and used not in result:
                    result[used] = by_name[used]

        return list(result.values())

===================================================================================================

from pathlib import Path
from typing import Dict, Any

from .blueprint_converter import BlueprintSpringBootConverter
from .doc_generator import load_file_doc  # or your existing helper
from .config import DOCS_DIR

def convert_file_to_spring(file_name: str) -> Dict[str, Any]:
    """
    Used by the UI when user clicks 'Generate Target Code' for a given
    legacy Java file. Assumes documentation JSON already exists.
    """
    doc_path = DOCS_DIR / f"{Path(file_name).stem}.json"
    if not doc_path.exists():
        raise FileNotFoundError(f"Documentation not found: {doc_path}")

    file_doc = load_file_doc(doc_path)
    converter = BlueprintSpringBootConverter()
    blueprint, java_files = converter.convert_file(
        file_doc=file_doc,
        original_file_name=file_name,
        domain_name="ChequeProcessing",
    )

    # you can also persist blueprint if you want
    bp_dir = DOCS_DIR / "blueprints"
    bp_dir.mkdir(parents=True, exist_ok=True)
    (bp_dir / f"{Path(file_name).stem}_blueprint.json").write_text(
        json.dumps(blueprint, indent=2), encoding="utf-8"
    )

    return {
        "blueprint": blueprint,
        "generated_files": [str(p) for p in java_files],
    }


========================================================================================4

from backend.pipeline import convert_file_to_spring


if st.button("Generate Target Code", key=f"code_{idx}"):
    try:
        result = convert_file_to_spring(file_name)
        gen_files = result["generated_files"]
        st.success(f"Generated {len(gen_files)} Spring Boot classes.")
        for p in gen_files:
            st.write(f"- {p}")
    except Exception as e:
        st.error(f"Code generation failed: {e}")


=============≈=================================≈=======================/

# backend/blueprint_converter.py
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Any, List

from .config import GENERATED_SRC_DIR
from .llm_client import chat_json


# ---------- SYSTEM PROMPT (STRICT) ----------

def _spring_system_prompt() -> str:
    """
    Very strict system prompt for Spring Boot conversion.

    We assume the model is strong (GPT-4.x / o3-mini) and can handle long,
    highly constrained instructions.
    """
    return (
        "You are a senior Spring Boot architect migrating a large legacy Java "
        "cheque-processing application to a modern, layered Spring Boot project.\n"
        "\n"
        "You are given a JSON payload with:\n"
        "  - base_package: the root Java package (e.g. com.bank.cheque).\n"
        "  - blueprint: overview of ALL classes in this legacy module, including\n"
        "               their responsibilities and relationships.\n"
        "  - target_class: documentation for ONE specific class that you must\n"
        "                  implement in Spring Boot.\n"
        "  - layer_hint: suggested application layer for target_class:\n"
        "       controller | service | repository | model | config | util.\n"
        "\n"
        "GENERAL RULES (MANDATORY):\n"
        "  1) You MUST generate a COMPLETE, COMPILABLE Java class for target_class.\n"
        "     - No method body may be empty.\n"
        "     - Do NOT return null just to satisfy types; implement realistic logic\n"
        "       or clear, step-by-step TODO comments that preserve all behaviour.\n"
        "  2) You MUST implement EVERY documented method for this class.\n"
        "     - All method names, parameters, and return types from the doc must\n"
        "       appear in the generated class.\n"
        "     - If the documentation describes validations, error handling, loops,\n"
        "       conditions, or business rules, implement them explicitly.\n"
        "  3) You MUST preserve cross-class behaviour.\n"
        "     - If the documentation says this class calls/depends on another\n"
        "       class, expose that as a Spring dependency:\n"
        "         - @Service classes injected into controllers/services.\n"
        "         - @Repository interfaces for persistence.\n"
        "         - @Component/@Service utilities where appropriate.\n"
        "  4) Use a LAYERED ARCHITECTURE:\n"
        "     - controllers: @RestController, @RequestMapping, public REST API,\n"
        "       convert input DTOs to domain types, delegate business logic to\n"
        "       services only.\n"
        "     - services: @Service, @Transactional as needed, encapsulate business\n"
        "       rules, call repositories and other services.\n"
        "     - repositories: @Repository / Spring Data JPA interfaces.\n"
        "     - model: @Entity, @Embeddable, DTOs, domain objects.\n"
        "     - config: @Configuration, @Bean definitions, property wiring.\n"
        "     - util: @Component or plain utility classes with reusable logic.\n"
        "  5) The code MUST be consistent with the blueprint:\n"
        "     - Use class names, roles, and relationships from the blueprint to\n"
        "       decide which collaborators to inject and how to call them.\n"
        "     - Do not invent random new classes; if you create helper DTOs or\n"
        "       enums, keep them simple and obviously related to the described\n"
        "       behaviour.\n"
        "  6) Package & imports:\n"
        "     - Always start with a correct package declaration derived from\n"
        "       base_package + logical layer folder.\n"
        "     - Import only what you use.\n"
        "     - Use modern Java 11 features where natural, but avoid exotic APIs.\n"
        "  7) Code style:\n"
        "     - Use clear naming.\n"
        "     - Add brief Javadoc for public methods summarising business\n"
        "       behaviour, especially where legacy design was complex.\n"
        "     - Prefer constructor injection (@RequiredArgsConstructor) over field\n"
        "       injection where possible.\n"
        "\n"
        "LAYER-SPECIFIC EXPECTATIONS:\n"
        "  - If layer_hint/controller:\n"
        "      * Use @RestController and @RequestMapping at class level.\n"
        "      * Expose endpoints covering ALL documented operations.\n"
        "      * Map method parameters to @RequestBody, @PathVariable, etc. as\n"
        "        appropriate.\n"
        "  - If layer_hint/service:\n"
        "      * Use @Service.\n"
        "      * Implement orchestration logic exactly as described in the docs,\n"
        "        calling other services/repositories.\n"
        "  - If layer_hint/repository:\n"
        "      * Use Spring Data JPA interface style where possible.\n"
        "      * Provide query methods that match documented lookups.\n"
        "  - If layer_hint/model:\n"
        "      * Use @Entity/@Table if the doc implies persistence.\n"
        "      * Model all fields, enums, and relationships from the docs.\n"
        "  - If layer_hint/config:\n"
        "      * Use @Configuration, expose Beans required by other classes.\n"
        "  - If layer_hint/util:\n"
        "      * Implement pure helper logic used by multiple classes.\n"
        "\n"
        "OUTPUT FORMAT (STRICT):\n"
        "  - You must output VALID JSON ONLY, no extra commentary.\n"
        "  - The JSON must have EXACTLY the following keys:\n"
        "        {\n"
        "          \"file_name\": \"<JavaFileName>.java\",   // may be omitted, then use <ClassName>.java\n"
        "          \"layer\": \"controller|service|repository|model|config|util\", // you may refine layer_hint\n"
        "          \"code\": \"<full Java source>\"         // with newlines escaped\n"
        "        }\n"
        "  - Do NOT wrap JSON in markdown fences.\n"
    )


# ---------- HELPER: LAYER GUESS ----------

def _guess_layer(class_name: str) -> str:
    name = class_name.lower()
    if any(k in name for k in ["controller", "menu", "resource", "endpoint"]):
        return "controller"
    if any(k in name for k in ["service", "manager", "processor", "handler"]):
        return "service"
    if any(k in name for k in ["repository", "repo", "dao"]):
        return "repository"
    if any(k in name for k in ["config", "configuration", "properties"]):
        return "config"
    if any(k in name for k in ["util", "utils", "helper"]):
        return "util"
    # default to model/domain if nothing matches
    return "model"


# ---------- BLUEPRINT CONVERTER ----------

@dataclass
class BlueprintSpringBootConverter:
    """
    Uses per-file documentation JSON to generate Spring Boot classes on a
    per-class basis, guided by a module-level blueprint.
    """
    file_doc: Dict[str, Any]
    base_package: str = "com.bank.cheque"

    # ---- blueprint building ----

    def _build_blueprint(self) -> Dict[str, Any]:
        """
        Build a compact blueprint summarising all classes in this file,
        for the LLM to understand cross-class relationships.
        """
        classes: List[Dict[str, Any]] = []
        for c in self.file_doc.get("classes", []):
            classes.append(
                {
                    "name": c.get("name"),
                    "description": c.get("description", ""),
                    "role": c.get("role"),  # optional high-level role if present
                    "methods": [
                        {
                            "name": m.get("name"),
                            "description": m.get("description", ""),
                            "returns": m.get("returns"),
                            "parameters": [
                                {
                                    "name": p.get("name"),
                                    "type": p.get("type"),
                                    "description": p.get("description", ""),
                                }
                                for p in m.get("parameters", [])
                            ],
                        }
                        for m in c.get("methods", [])
                    ],
                    "fields": [
                        {
                            "name": f.get("name"),
                            "type": f.get("type"),
                            "description": f.get("description", ""),
                        }
                        for f in c.get("fields", [])
                    ],
                    "relationships": c.get("relationships", []),
                    "depends_on": c.get("depends_on", []),
                    "inner_classes": [
                        ic.get("name") for ic in c.get("inner_classes", [])
                    ],
                }
            )
        return {
            "module_name": self.file_doc.get("module_name"),
            "module_description": self.file_doc.get("description", ""),
            "base_package": self.base_package,
            "classes": classes,
        }

    def _find_class_doc(self, class_name: str) -> Dict[str, Any]:
        for c in self.file_doc.get("classes", []):
            if c.get("name") == class_name:
                return c
        raise ValueError(f"Class {class_name!r} not found in file documentation.")

    # ---- main generation API ----

    def generate_for_class(self, class_name: str) -> Path:
        """
        Generate a single Spring Boot class for the given class name.

        Writes the .java file under GENERATED_SRC_DIR and returns the path.
        """
        class_doc = self._find_class_doc(class_name)
        layer_hint = _guess_layer(class_name)
        blueprint = self._build_blueprint()

        payload = {
            "base_package": self.base_package,
            "blueprint": blueprint,
            "target_class": class_doc,
            "layer_hint": layer_hint,
        }

        resp = chat_json(_spring_system_prompt(), payload, max_tokens=3500)

        code: str = resp["code"]
        layer: str = resp.get("layer") or layer_hint
        file_name: str = resp.get("file_name") or f"{class_name}.java"

        # Normalise layer into a subfolder
        layer_folder = {
            "controller": "controller",
            "service": "service",
            "repository": "repository",
            "model": "model",
            "config": "config",
            "util": "util",
        }.get(layer.lower(), "service")

        rel_dir = Path(*self.base_package.split(".")) / layer_folder
        out_dir = GENERATED_SRC_DIR / rel_dir
        out_dir.mkdir(parents=True, exist_ok=True)

        out_path = out_dir / file_name
        out_path.write_text(code, encoding="utf-8")
        return out_path


# ---------- OPTIONAL: generate all classes for a file ----------

def convert_file_to_spring(
    file_doc: Dict[str, Any],
    base_package: str = "com.bank.cheque",
) -> Dict[str, Path]:
    """
    Convenience helper if you ever want to generate ALL classes for a file
    in one shot. Not used directly by app.py but you can call it from
    experiments or batch scripts.
    """
    conv = BlueprintSpringBootConverter(file_doc, base_package=base_package)
    result: Dict[str, Path] = {}
    for c in file_doc.get("classes", []):
        name = c.get("name")
        if not name:
            continue
        result[name] = conv.generate_for_class(name)
    return result

==========================================================


# backend/pipeline.py
from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, Any

from .ast_extractor import run_java_extractor, load_ast
from .doc_generator import generate_documentation_for_file
from .test_generator import generate_tests_for_spring_class
from .config import AST_JSON_PATH, DEFAULT_LEGACY_REPO
from .json_store import load_state, save_state
from .blueprint_converter import BlueprintSpringBootConverter


def ensure_ast(repo: Path = DEFAULT_LEGACY_REPO) -> Dict[str, Any]:
    """
    Make sure backend/data/ast.json exists, otherwise run the Java extractor.
    """
    if not AST_JSON_PATH.exists():
        return run_java_extractor(repo, AST_JSON_PATH)
    return load_ast(AST_JSON_PATH)


def generate_docs(file_name: str, repo: Path = DEFAULT_LEGACY_REPO) -> Dict[str, Any]:
    """
    Full pipeline for a given legacy .java file:
    - ensure AST
    - chunk + generate documentation per chunk
    - collate into per-class documentation
    - persist JSON under backend/docs/<file_stem>.json
    - update status in json_store
    """
    ast = ensure_ast(repo)
    file_path = repo / file_name
    file_doc = generate_documentation_for_file(ast, file_path)

    meta = load_state()
    fmeta = meta.setdefault(file_name, {})
    fmeta["doc_status"] = "generated"
    save_state(meta)

    return file_doc


def convert_class_to_spring(
    file_name: str,
    class_name: str,
    repo: Path = DEFAULT_LEGACY_REPO,
):
    """
    Convert a single documented class to Spring Boot, using the
    BlueprintSpringBootConverter.  This is what your Streamlit UI calls.
    """
    from .doc_generator import DOCS_DIR  # local import to avoid circulars

    # Ensure AST exists (not strictly needed here but keeps the pipeline consistent)
    ensure_ast(repo)

    doc_path = DOCS_DIR / f"{Path(file_name).stem}.json"
    if not doc_path.exists():
        raise RuntimeError("Documentation not found; generate docs first.")

    file_doc = json.loads(doc_path.read_text(encoding="utf-8"))

    # Use the blueprint-aware converter instead of the older generate_spring_for_class
    converter = BlueprintSpringBootConverter(file_doc, base_package="com.bank.cheque")
    spring_path = converter.generate_for_class(class_name)

    # Persist status so the UI table shows 'Generated'
    meta = load_state()
    fmeta = meta.setdefault(file_name, {})
    classes_meta = fmeta.setdefault("classes", {})
    cmeta = classes_meta.setdefault(class_name, {})
    cmeta["spring_path"] = str(spring_path)
    cmeta["spring_status"] = "generated"
    save_state(meta)

    return spring_path


def generate_tests(
    file_name: str,
    class_name: str,
    repo: Path = DEFAULT_LEGACY_REPO,
):
    """
    Generate JUnit tests for a given class, using the previously-generated
    Spring Boot code and the class-level documentation.
    """
    from .doc_generator import DOCS_DIR  # local import to avoid circulars

    doc_path = DOCS_DIR / f"{Path(file_name).stem}.json"
    if not doc_path.exists():
        raise RuntimeError("Documentation not found; generate docs first.")

    file_doc = json.loads(doc_path.read_text(encoding="utf-8"))
    clazz = next(c for c in file_doc["classes"] if c["name"] == class_name)

    meta = load_state()
    cmeta = meta[file_name]["classes"][class_name]
    spring_path = Path(cmeta["spring_path"])

    test_path = generate_tests_for_spring_class(spring_path, clazz)

    cmeta["test_path"] = str(test_path)
    cmeta["test_status"] = "generated"
    save_state(meta)

    return test_path


==============

def collect_full_source_for_class(class_name: str) -> str:
    """
    Reconstruct full legacy source for a class by merging
    all chunks where this class appears.
    """
    from .chunker import load_chunk_db

    chunks = load_chunk_db()
    related = [
        c for c in chunks
        if class_name in c.get("classes", [])
    ]

    related.sort(key=lambda c: c["start_line"])
    return "\n\n".join(c["source_code"] for c in related)


====================================================≈================

# --- merge fields ---
existing_fields = {f.get("name") for f in master["fields"]}
for f in c.get("fields", []):
    if f.get("name") not in existing_fields:
        master["fields"].append(f)

# --- merge class dependencies ---
for dep in c.get("uses_classes", []):
    master["uses_classes"].add(dep)

for ub in c.get("used_by_hint", []):
    master["used_by_hint"].add(ub)

notes = (c.get("notes") or "").strip()
if notes and len(notes) > len(master.get("notes", "")):
    master["notes"] = notes



master.pop("_methods_index", None)

# convert sets → lists for JSON
master["uses_classes"] = sorted(master.get("uses_classes", []))
master["used_by_hint"] = sorted(master.get("used_by_hint", []))

======================≈=========================================================================
# legacy_to_spring_app.py
# END-TO-END LEGACY JAVA → SPRING BOOT (RAG + REFINED DOC + UI)

import streamlit as st
import javalang
from pathlib import Path
from typing import List, Dict, Any
import json

# =========================================================
# 1. JAVA PARSING (AST + INNER CLASSES)
# =========================================================

def parse_java(java_code: str) -> Dict[str, Any]:
    tree = javalang.parse.parse(java_code)
    classes = []

    def walk_class(node, outer=None):
        cls = {
            "name": node.name,
            "outer_class": outer,
            "fields": [],
            "methods": [],
            "inner_classes": []
        }

        for f in node.fields:
            for d in f.declarators:
                cls["fields"].append({
                    "name": d.name,
                    "type": str(f.type)
                })

        for m in node.methods:
            cls["methods"].append({
                "name": m.name,
                "parameters": [p.name for p in m.parameters],
                "returns": str(m.return_type) if m.return_type else "void"
            })

        for inner in node.body:
            if isinstance(inner, javalang.tree.ClassDeclaration):
                cls["inner_classes"].append(inner.name)
                classes.append(walk_class(inner, node.name))

        return cls

    for path, node in tree:
        if isinstance(node, javalang.tree.ClassDeclaration):
            classes.append(walk_class(node))

    return {"classes": classes}


# =========================================================
# 2. CHUNKING (LOGICAL, NOT LINE BASED)
# =========================================================

def chunk_java(java_code: str) -> List[str]:
    lines = java_code.splitlines()
    size = 120
    overlap = 30
    chunks = []
    i = 0
    while i < len(lines):
        chunks.append("\n".join(lines[i:i+size]))
        i += size - overlap
    return chunks


# =========================================================
# 3. CHUNK-LEVEL DOCUMENTATION (LLM SIMULATION)
# =========================================================

def document_chunk(chunk: str, ast: Dict[str, Any]) -> Dict[str, Any]:
    """
    In production this is an LLM call.
    Here we deterministically extract semantics.
    """
    return {
        "classes": ast["classes"]
    }


# =========================================================
# 4. REFINED DOCUMENT (LOSSLESS SEMANTIC MODEL)
# =========================================================

def build_refined_document(chunk_docs: List[Dict[str, Any]]) -> Dict[str, Any]:
    merged = {}

    for cd in chunk_docs:
        for c in cd["classes"]:
            name = c["name"]
            master = merged.setdefault(name, {
                "name": name,
                "outer_class": c["outer_class"],
                "fields": [],
                "methods": [],
                "inner_classes": [],
                "responsibility": "",
                "invariants": [],
                "dependencies": []
            })

            for f in c["fields"]:
                if f not in master["fields"]:
                    master["fields"].append(f)

            for m in c["methods"]:
                if m not in master["methods"]:
                    master["methods"].append({
                        **m,
                        "steps": [
                            f"Validate inputs for {m['name']}",
                            f"Execute core logic for {m['name']}",
                            f"Return result"
                        ],
                        "side_effects": ["database write"] if "save" in m["name"].lower() else []
                    })

            for ic in c["inner_classes"]:
                if ic not in master["inner_classes"]:
                    master["inner_classes"].append(ic)

    return {
        "document_type": "REFINED_CLASS_SPEC",
        "classes": list(merged.values())
    }


# =========================================================
# 5. BLUEPRINT GENERATION
# =========================================================

def build_blueprint(refined_doc: Dict[str, Any]) -> Dict[str, Any]:
    blueprint = []
    for c in refined_doc["classes"]:
        blueprint.append({
            "class": c["name"],
            "role": "service",
            "methods": [m["name"] for m in c["methods"]],
            "inner_classes": c["inner_classes"]
        })
    return {"blueprint": blueprint}


# =========================================================
# 6. SPRING BOOT CODE GENERATION (DOC ONLY)
# =========================================================

def generate_spring_code(class_doc: Dict[str, Any]) -> str:
    lines = []
    pkg = "com.bank.cheque.service"

    lines.append(f"package {pkg};\n")
    lines.append("import org.springframework.stereotype.Service;\n")

    lines.append("@Service")
    lines.append(f"public class {class_doc['name']}Service {{")

    for f in class_doc["fields"]:
        lines.append(f"    private {f['type']} {f['name']};")

    for m in class_doc["methods"]:
        params = ", ".join([f"String {p}" for p in m["parameters"]])
        lines.append(f"\n    public {m['returns']} {m['name']}({params}) {{")
        for step in m["steps"]:
            lines.append(f"        // {step}")
        if m["returns"] != "void":
            lines.append("        return null;")
        lines.append("    }")

    for ic in class_doc["inner_classes"]:
        lines.append(f"\n    private static class {ic} {{")
        lines.append("        // inner class logic preserved")
        lines.append("    }")

    lines.append("}")
    return "\n".join(lines)


# =========================================================
# 7. STREAMLIT UI (UNCHANGED UX)
# =========================================================

st.set_page_config(page_title="Legacy → Spring Boot", layout="wide")
st.title("Legacy Java → Spring Boot (RAG + Refined Docs)")

uploaded = st.file_uploader("Upload Legacy Java File", type=["java"])

if uploaded:
    java_code = uploaded.read().decode("utf-8")
    st.subheader("Legacy Source")
    st.code(java_code, language="java")

    if st.button("Generate Documentation"):
        ast = parse_java(java_code)
        chunks = chunk_java(java_code)

        chunk_docs = []
        for ch in chunks:
            chunk_docs.append(document_chunk(ch, ast))

        refined_doc = build_refined_document(chunk_docs)
        st.session_state["refined_doc"] = refined_doc

        st.success("Refined documentation generated")

    if "refined_doc" in st.session_state:
        st.subheader("Refined Documentation")
        st.json(st.session_state["refined_doc"])

        if st.button("Show Blueprint"):
            blueprint = build_blueprint(st.session_state["refined_doc"])
            st.subheader("Blueprint")
            st.json(blueprint)

        if st.button("Generate Spring Boot Code"):
            for c in st.session_state["refined_doc"]["classes"]:
                code = generate_spring_code(c)
                st.subheader(f"Spring Code: {c['name']}")
                st.code(code, language="java")
